\documentclass{acm}

% Pseudocode
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\usepackage[justification=centering, labelfont=bf]{caption}

% Spacing between paragraphs
\newcommand{\subparagraph}{}
\usepackage{titlesec}
\titlespacing*{\section}{0pt}{1em}{1em}
\titlespacing*{\subsection}{0pt}{1em}{1em}
\titlespacing*{\subsubsection}{0pt}{1em}{1em}

\title{Write-Optimised Log-Structured Merge Tree}
\subtitle{CS 265 Final Project, Spring '17}
\author{Jack Dent\\\email{jdent@college.harvard.edu}}

\begin{document}

\maketitle

\begin{abstract}
This work presents the design a write-optimised Log-Structured Merge Tree (LSM Tree), which serves as a standalone key-value store, as well as an implementation in C++. Various parameters give users given fine grained control over the tradeoff between the store's read and write throughput, and lookup query parallelisation allows the store to increase the saturation of available hardware, thereby improving performance. Experimental results show that the store is able to achieve a write throughput of 250K and a read throughput of 50K on the test machine for a set of commonly used parameter values.
\end{abstract}

\section{Introduction}

The LSM Tree has seen increasing adoption as an industry standard for modern key-value stores, both as standalone products and as components in larger database management systems (DBMSs). Facebook's RocksDB, Google's LevelDB, and Amazon's Dynamo are prominent examples of standalone key value stores that depend on LSM Trees as their primary storage layer. SQLite4, meanwhile, dropped the B-tree in favour of the LSM Tree as the main table store. By avoiding in-place updates and instead buffering all writes in main-memory, the LSM Tree vastly improves write throughput by amortising costly roundtrips to and from secondary storage over very many write operations. When the main-memory buffer becomes full, the LSM Tree flushes its entire contents to secondary storage. On disk, the LSM Tree stores a number of levels, each of which contain a number of ``runs'' whose size increases in proportion to the level depth. To increase the performance of point and range queries, the LSM Tree stores a number of auxiliary data structures in main-memory for each run. Bloom filters allow point queries to avoid searching runs that are known to not contain the search key. Fence pointers, meanwhile, establish bounds on the the subranges in run pages, allowing point and range queries to avoid having to read inconsequential pages into main-memory.

This paper presents the design and implementation of a parallelised LSM Tree that allows application users to control the tradeoff between read and write throughput by tuning the following parameters during initialization:

\begin{enumerate}
\item \textbf{Buffer size, $b$:} controls the number of pages in the main-memory buffer, and therefore also affects the size of runs on secondary storage. Defaults to 1000, which gives the buffer a size of 4MB.
\item \textbf{Level fanout, $f$:} controls the size ratio between the levels. All levels can contain a maximum of $f$ runs, where the maximum size of the run increases exponentially in $f$. For example, the first level contains runs of size $bf$, the second level $bf^2$, and so forth. As will be discussed, the level fanout controls the tradeoff between read and write throughput. Defaults to 10.
\item \textbf{Tree depth, $d$:} controls the number of levels on disk, and as such the overall storage capacity of the tree. Defaults to 5.
\item \textbf{Thread count, $t$:} controls the number of worker threads dispatched to process queries in parallel. Defaults to 4.
\item \textbf{Bloom filter bits per entry, $r$:} controls the number of bits each entry receives in the bloom filter. Defaults to 0.5, which means that the bloom filter is two orders of magnitude smaller than the array of entries.
\end{enumerate}

\section{Design}

The LSM Tree has two primary components: an in-memory buffer, and a number of ``levels'' on secondary storage of progressively increasing size. For the purposes of this paper, a ``key/value entry'' is an compact C \texttt{struct} that contains two 4-byte integers, a key and a value, and whose total size is therefore $e=8$ bytes.

\subsection{Buffer design}

The ``buffer'' is an in-memory \texttt{std::set} of entry \texttt{structs}. The parameter $b$ controls the number of pages in the buffer, and the number of entries is thus given by $bp/e$, where $p$ is the size of a single page in bytes. Storing a buffer as a unsorted log would ensure optimal $O(1)$ write performance, but would require $O(bp/e)$ operations for point queries. On the other hand, storing the buffer as a sorted array would improve the performance of point queries to $O(\log(bp/e)$, but every insertion would cost $O(bp/e)$.

To balance the performance of reads and writes I used the \texttt{std::set} data structure, which implements a binary search tree and demonstrated a time complexity of $\Theta(\log(bp/e))$ for both lookup and insertion. Under most circumstances, the buffer will not be the bottleneck for workload throughput since the cost of secondary storage I/O will dominate query latencies. Under workloads that exhibit highly skewed query distributions or strong time contiguity within wider distrbutions, the buffer will again become important since the majority of queries will never make it to disk, and in this case a skip list will be most appropriate. Rather than attempting to optimise the buffer data structure, this project chose to focus on optimisations that applied across a broader range of workload distributions.

\subsection{Level design}

LSM Tree implementations can be divided into two categories based on their choice of merge policy. The first merge policy, known as \textit{leveling}, stores entire levels as single files that contain sorted key/value entries on disk. Leveled LSM Trees require a merge/sort between the buffer and first disk level every time the buffer becomes full, and also requires that entire levels are merge/sorted when upper levels become full. Since level size increases with depth, merging two levels that potentially contain millions of entries can take a significant amount of time unless the fanout $f$ is low, during which all queries to the tree are blocked. Thus, leveled LSM Trees are primarily optimised for high lookup throughput, rather than high update throughput. On the other hand, a \textit{tiered} LSM Tree makes updates less costly at the expense of reads. A tiered LSM Tree splits levels into multiple runs, and therefore avoids having to merge levels every time the buffer becomes full. Lookup queries become more expensive, however, since levels are no longer sorted, only the runs. As $f \rightarrow \infty$, a leveled LSM Tree becomes a sorted array and a tiered LSM Tree becomes a log.

\subsubsection{Runs}

I opted to build a write-optimised LSM Tree, and therefore opted to implement the tiering merge policy. In my design, a ``level'' holds a time-ordered collection of ``runs'' stored in a \texttt{std::deque}. A ``run'' is an in-memory data structure that holds a pointer to a file stored on disk, a fixed-size bloom filter, and fence pointers. The file stores a number of entries in ascending order, sorted by key. In the case of a crash, the store can recreate the bloom filter and the fence pointers for all runs from the entries on disk, although any entries in the buffer that have not been checkpointed will be lost. Every level contains a maximum of $f$ runs at any one time, and the maximum size of each run at level $l$ (in pages) is bounded by $O(bf^l)$. A run is not guaranteed to contain the maximum number of entries since the merge procedure removes duplicates, but runs in the same level will all have approximately the same size. The store never updates existing runs in place.

\subsubsection{Run ordering}

The store induces a global time-ordering across all runs, which fixes a pseudo-logical timestamp for each run. The global ordering is given first by level depth, and then by the position of the run within that level, where the most recent runs are stored at the front of the level \texttt{deque}. A single run will never contain more than one entry for a given key; on the other hand, there is no guarantee that different runs will not contain conflicting entries. The global ordering helps to ensure that the LSM Tree always gives preference to the most recent key/value pairing when searching and merging levels.

\subsubsection{Bloom filters}

Bloom filters improve the performance of point queries by reducing the number of secondary storage I/Os needed to search a run. When the store writes entries to a new run, it marks each entry as present in the bloom filter by hashing the key using three different functions and then setting the bit at each of the hashed positions to the value of 1 in a bitmask. For a given key, a bloom filter probe returns a negative result if any one of the three bits at the hashed positions of that key in the bitmask is 0. Since bloom filters allow false positives, the store will sometimes perform one unnecessary read I/O during GET queries in the case where the bloom filter incorrectly implies a relevant is present in the run on disk. Importantly, bloom filters never return false negatives, which implies that the store will never fail to discover an entry that actually exists within a disk level.

\subsubsection{Fence pointers}

Fence pointers improve the performance of point and range queries by providing information about the location of entries on disk, thus preventing the store from having to perform an expensive binary seek over the entire file. Each run stores fence pointers as a \texttt{std::vector} of keys, where the number of fence pointers is equal to the number of pages in the run, and the fence pointer at index $i$ contains the value of the first key in page $i$ of the run. Each run also stores an additional fence pointer that contains the maximum key over all its entries, which establishes an upper bound on the key range of the last page.

\begin{figure}
\includegraphics[width=3.33in]{merge}
\caption{The tiered merge policy for an LSM Tree with $f=2$, $d=3$, where the buffer contains exactly key/value entries. When the buffer (level B) becomes full, the LSM Tree must flush its entries to level 1 to make space for future PUT queries. However, since level 1 is full, the merge down procedure must first flush level 1 to level 2 (step 1). Since the first run in level 1 contains the entry 4:7 and the second contains 4:12, the \texttt{MergeSort} procedure ensures that only the more recent value, 4:7, is written to the third level. As such, the first run in the third level only contains 5 entries, although it has a capacity of $b \cdot f^{l-l} = 3 \times 2^1 = 6$. After completing step 1, the LSM Tree flushes the buffer to level 1, which is now empty.}
\label{fig:merge}
\end{figure}

\subsection{PUT queries}

The LSM Tree always inserts new key/value pairs into the buffer. If the buffer is full then the store will flush the buffer to disk before attempting to insert the key. If the first level is full, in that it contains exactly $f$ runs, the store will call \texttt{MergeDown(1)} before attempting to flush the buffer to the first level. When the \texttt{MergeDown} procedure terminates (if invoked), the LSM Tree creates a new run at the start of the first level, inserts every entry in the buffer setting the relevant bloom filter entries and fence pointers, and then empties the buffer. Thus, PUT queries always create at least one new run if the buffer is full.

\subsubsection{\texttt{MergeDown} policy}

The \texttt{MergeDown($l$)} procedure for clearing space in disk levels works as follows. First, if the current level $l$ already has capacity for a new run, the procedure terminates (line 4). If level $l+1$ does not have space for the entries in level $l$, it recursively calls \texttt{MergeDown($i+1$)} (lines 5-6). At line 7, it is guaranteed that level $l+1$ will contain at most $f-1$ runs, and also that a run in level $l+1$ has the capacity to store all $f$ runs in level $l$. The procedure then performs a merge/sort on all the runs in level $l$ (line 7) and writes the results to a new run at the start of level $l+1$ (line 8). The final stage of the procedure clears level $l$ of all runs and entries, since they will now be contained by level $l+1$.

\begin{algorithm}[b]
\caption{Merge down procedure}
\begin{algorithmic}[1]
\Procedure{MergeDown($l$)}{}
\State $n_l \gets \text{number of runs in level } l$
\State $n_{l+1} \gets \text{number of runs in level } l+1$
\If {$n_l < f$}
\Return
\EndIf
\If {$n_{l+1} = f$}
\State $\Call{MergeDown}{l+1}$
\EndIf
\State $r_{new} = \Call{MergeSort}{\text{runs in level } l}$
\State $\text{add } r_{new} \text{ to level } l + 1$
\State $\text{empty level } l$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsubsection{\texttt{MergeSort}}

To merge $f$ runs, the LSM Tree invokes an $f$-way external \texttt{MergeSort} procedure. \texttt{MergeSort} is also responsible for both removing duplicate keys and pruning deleted entries from the final level. \texttt{MergeSort} pushes the first entry in each of the levels and the file pointer associated with that disk level onto a priority queue. The queue contains tuples of the form $(entry, timestamp, file)$ ordered first by the entry key and second by timestamp, which ensures that the store resolves key collisions in the correct order. While the priority queue is not empty, the procedure pops the first tuple and writes the entry to the new run if two conditions hold:

\begin{itemize}
  \item The current key is different to the last. If the current entry contains the same key as the previously seen entry, it is discarded to prevent duplicates. Since the priority queue gives precedence to runs with smaller logical timestamps in the case of key conflicts, \texttt{MergeSort} will always write the more recent entry to the new run.duplication.
  \item It is not the case that the new run is in the final level and the current entry is marked as deleted. Thus, we eventually remove deleted entries from the tree.
\end{itemize}

The \texttt{MergeSort} procedure makes sure both to set the entry's bits in the bloom filter, and to update the fence pointers in the new run whenever it writes an entry to disk. After writing an entry to disk, \texttt{MergeSort} checks to see if the file stream has been exhausted. If the file has remaining entries, the procedure pushes the new $(entry', timestamp, file)$ tuple back onto the priority queue, repeating this entire procedure until the queue is empty.

\subsubsection{Complexity}

A lower value of $f$ enforces a stricter global ordering over the entries and thus improves read performance at the expense of higher amortized writes costs. On the other hand, a higher value of $f$ reduces the frequency of the merge operation and therefore improves write throughput while penalising reads. The worst-case workload for updates occurs when all inserted keys are eliminated, which implies that all runs will have maximum capacity at all levels. Since there are at most $d$ levels in the tree, and since each key/value entry is merged at most once per level, entries are subject to at most $O(d)$ merge/sort operations. The write I/O cost of merging a single entry is $1/b$, and the amortized worst case I/O write complexity of the update procedure is therefore $O(\frac{d}{b})$. The \texttt{MergeDown} algorithm reads exactly as many pages as it writes in the worst case run of update operation, and assuming that the ratio of read I/O cost to write I/O cost is $\mu$, the total amortized I/O cost is therefore $O(\frac{d}{b} + \mu \cdot \frac{d}{b}) = O((1 + \mu) \cdot \frac{d}{b})$. After $n$ writes, the tree has depth at most $\log_f(n/b)$, which implies the update complexity is $O(\frac{1}{b} \cdot (1 + \mu) \cdot \log_f(n/b))$. Thus, the update cost decreases as $f$ increases, and when $f=n/b$, the insertion cost becomes $1/b \cdot (1 + \mu)$, which turns the store into a log.

\subsubsection{Parallelisation}

A typical value for the level fanout $f$ parameter used by many commercial LSM Trees (including LevelDB) is 10. With this setting, each level will contain at most 10 runs, and the merge procedure will always flush 10 runs to the next level when the current level reaches its capacity. Since this work used a machine with 4 cores, the multi-threaded \texttt{MergeSort} procedure did not exhibit a significant advantage over a single threaded approach.

Although this project did not experiment with larger values of $f$, the following procedure could improve the performance of PUT queries through parallelisation in those circumstances. When $f < c$, where $c$ is a tunable parameter, the LSM Tree should fall back to merging the entries in a single thread. Otherwise, the LSM Tree should uniformly partition the runs into subranges, and each thread should merge exactly one subrange, buffering the resulting array in main-memory. The main thread should wait for all of the threads to finish their merge procedures and then perform a $t$-way merge/sort over the buffers, writing the result to a new run in the appropriate level. As an alternative, a number of (potentially preemptable) background daemon threads could proactively merge together runs in levels with low capacity.

\subsection{GET queries}

The single-threaded store answers point queries by first searching the buffer, and then only searching the lower levels if no relevant entry was found. Searching the buffer does not perform an I/O operations and therefore has negligible cost unless the workload is heavily skewed, as discussed above (see ``Buffer design'' section). To search the lower levels, the LSM Tree searches every run in every level according to the global time-ordering.

To search for a key in a single run, the store first checks that the key is present in the bloom filter, and also that it falls within the fence pointer range. If either probe returns a negative, it is certain that the run file does not contain an entry for the specified key and the search can continue in the next run. Otherwise, the procedure performs a binary search over the fence pointer \texttt{vector} to find the index of the page it should read from secondary storage. By specifying a unique page for a given key within each run, the fence pointers thus allow the search procedure to avoid performing a logarithmic number of read I/Os to identify the relevant page. Finally, the procedure reads a single page into a buffer and attempts to find an entry for the specified key, continuing to the next run if no such entry could be found. If the procedure moves past the final run without finding a key, it concludes that no relevant entry exists. 

\subsubsection{Complexity}

To model the cost of GET queries, we first note that querying a missing key will require a full search over every level of the tree and that as such, worst case performance will be fairly common under many workloads. Thus, we model the complexity of GET queries by assuming a probe to every run.

Searching a run for a key takes at most one read I/O cost due to the fence pointers, and since each level contains $f$ runs, the total I/O cost of searching run $l$ is $O(f)$. However, the store only performs an unnecessary read I/O if the bloom fitler probe returns a false positive, since it stops the search as soon as it finds a run that run actually contains the entry. If we assume a constant false positive rate $\phi$ across all bloom filters, which can be tuned by the bits per entry parameter $r$, we only perform $O(\phi \cdot f)$ unnecessary read I/Os per level in the worst case. As such, the worst case I/O cost of searching all levels is $O\left(\phi \cdot f \cdot d\right)$. After $n$ writes, the tree has depth at most $\log_f(n/b)$, which implies the lookup complexity is $O(\phi \cdot f \cdot \log_f(n/b))$. Thus, the lookup cost increases as $f$ increases, and when $f=n/b$, the lookup cost becomes $\phi \cdot n/b$.

\subsubsection{Parallelisation}

To parallelise GET queries, the store uses $t$ threads to search $t$ runs simultaneously. Each thread has access to the four variables in shared memory: a spinlock to enforce mutual exclusion, a run counter that is initially set to 0, a integer id that is initially set to -1, and a value variable. Threads execute the following loop to search runs.

First, the thread stores the current value of the counter in a private variable, and then atomically increments the value of the shared counter using the spinlock. If the integer id is still -1, or if the private counter variable is greater than the total number of runs over the entire tree, the thread ends its search loop. Otherwise, the thread searches the run whose timestamp is equal to the private counter variable by following the procedure outlined in the single threaded approach. If no entry was found in the run, the thread executes the next iteration of the loop. On the other hand, if the run probe discovered an entry, the thread acquires the spinlock and updates the shared value and integer id variables to the discovered entry's value and the private counter respectively. The main thread waits for all threads to end their loops.

Intuitively, threads keep searching for entries until another thread has discovered a relevant entry, or until they have either exhausted the tree. The shared integer id is used to ensure that the entries in more recent runs are always given precedence than those further down the tree. Barring thread management overhead, the speed of GET queries should increase in proportion to the number of threads up to the optimal value for the hardware.

\subsection{DELETE queries}

DELETE queries are handled almost identically to PUT queries. To delete a key, the LSM Tree inserts the key with a tombstone value. By restricting the range of possible values to $[-2147483647, 2147483647]$, it is possible to assign $-2147483648$, the minimum value of a 32 bit integer, as the tombstone. Alternatively, if we did not want to restrict the value range, we could instead increase the size of the entries stored by the tree by an additional byte, which would indicate whether the deletion status of the associated key. Another option would be to store a deletion list in each run.

We keep tombstone entries until they reach the final level. When an entry reaches the final level, it is guaranteed that there are no deeper levels containing outdated values for that key which allows the store to remove tombstone entries, thereby preventing the tree from becoming saturated with deleted key/value pairings.

\subsection{RANGE queries}

The single-threaded store searches for subranges in the buffer and each run independently, buffers the results into main-memory vectors, and then merge/sorts the subrange vectors according to the global time ordering.

To search for a subrange in the buffer, the store performs a binary search to find the lower and upper bounds, copies the results into a new \texttt{vector}, and pushes that \texttt{vector} onto a queue.

The procedure then loops over the runs in order, building a subrange \texttt{vector} for each. To search for a subrange in a single run, the store performs two binary searches over the fence pointers to find the indices of the start and end pages for the given subrange, and then reads the contiguous set of pages between these indices into main memory. The store then filters the array of entries to the specified subrange and pushes the resulting vector onto a queue.

\subsubsection{Complexity}

The cost of RANGE queries depends entirely on the parameter values for the start and end keys. For example when the start key is minimised and the end key is maximised, the store will have to perform a read I/O for every single page in the tree to collect all the values.

Assuming that the start key is $k_s$ and the end key is $k_s + \Delta$, the range can span at most $ \frac{e \cdot \Delta}{p}$, and the store will thus read at most $O(\frac{e \cdot \Delta}{p})$ pages to find the subrange for a given run. The total I/O cost of searching a level is therefore $O(f \cdot frac{e \cdot \Delta}{p})$. It follows that the complexity of RANGE queries is $O(d \cdot f \cdot frac{e \cdot \Delta}{p})$. When $\Delta < p$, the page size, the complexity equals that of a point query scaled by a factor of $\phi$, since the only difference between the queries is in the use of bloom filters.

\subsubsection{Parallelisation}

To parallelise RANGE queries, the store uses $t$ threads to find subranges in $t$ runs simultaneously. Each thread has access to the three variables in shared memory: a spinlock to enforce mutual exclusion, an atomic counter that is initially set to 0, and a C++ \texttt{map} that associats run timestamps with subranges. Threads execute the following loop to search runs.

First, the thread stores the current value of the counter in a private variable, and then atomically increments the value of the shared counter. If the private counter variable is greater than the total number of runs over the entire tree, the thread ends its search loop. Otherwise, the thread finds the subrange in the run whose timestamp is equal to the private counter variable by following the procedure outlined in the single threaded approach. Finally, the thread acquires the spinlock and associates the discovered subrange with the private counter variable in the map. 

The main thread waits for all threads to end their loops and then merge/sorts the in-memory subrange buffers, giving precedence to subranges with lower ids when resolving merge conflicts. Barring thread management overhead, the speed of GET queries should increase in proportion to the number of threads up to the optimal value for the hardware.

\subsection{LOAD queries}

LOAD queries stream a binary file of key/value pairs, inserting each into the store in the sequence they occur. Thus, a LOAD query is almost identical to a series of independent PUT queries, with the one exception that it parses binary rather than ASCII data.

\section{Optimisations}

\subsection{Thread pool}

A ``thread pool'' is a reusable pool of kernel threads. We use a threadpool for parallelisation so that we do not have to create a new set of threads for each query. The number of threads in the threadpool is denoted $t$ and should be tuned to optimal value for the hardware, which will often be the number of cores on the machine.

\subsection{mmap}

TODO

\section{Experimental results}

\begin{figure}
\centering
\includegraphics[width=3.33in]{fanout}
\caption{Effect of level fanout on PUT query throughput when $b=1$}
\end{figure}

Following parameters used: $b = 100$; $t = 4$; $f = 10$.

\subsection{Effect of varying $f$}

\subsection{Effect of varying $r$}

\subsection{Thread pool}


% Figure 1 depicts how the performance characteristics of a number of different workloads vary according to the buffer size. I tested a number of different workloads with various distrubutions for each page size, and then averaged the time taken to complete the workload. The time axis is logarithmic.

\section{Conclusion and future work}

- Implement skip list; buffer is bottleneck
- Checkpointing buffer without a full cascading merge -- just persist entire buffer to disk

\end{document}
